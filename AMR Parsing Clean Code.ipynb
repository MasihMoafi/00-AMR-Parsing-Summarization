{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0dbb6-50f7-404c-b6f6-55fd83258267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Intuition \n",
    "\n",
    "'''\n",
    "graph TD\n",
    "  A[Original Text] --> B(AMR Parsing)\n",
    "  B --> C{AMR Graph}\n",
    "  C --> D[Graph Linearization]\n",
    "  D --> E[Graph Tokenization]\n",
    "  E --> F[BERT Embedding]\n",
    "  F --> G[Model Input]\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0925bfc4-7c61-4c30-a415-cb48b596c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing AMR graphs...\n",
      "Epoch: 1, Batch: 0, Loss: 7.6050\n",
      "Epoch: 1, Batch: 1, Loss: 7.6442\n",
      "Epoch 1 Average Loss: 7.6246\n",
      "Epoch: 2, Batch: 0, Loss: 7.3074\n",
      "Epoch: 2, Batch: 1, Loss: 7.4173\n",
      "Epoch 2 Average Loss: 7.3623\n",
      "Epoch: 3, Batch: 0, Loss: 7.0976\n",
      "Epoch: 3, Batch: 1, Loss: 7.1774\n",
      "Epoch 3 Average Loss: 7.1375\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: AS2SP Model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import amrlib\n",
    "\n",
    "# SAMPLE DATA GENERATION \n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Scientists discover a new species of dinosaur in Argentina.\",\n",
    "        \"Global tech conference announces 2024 event location as Singapore.\",\n",
    "        \"New study shows coffee consumption linked to improved heart health.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"New dinosaur species found in Argentina.\",\n",
    "        \"Tech conference 2024 to be held in Singapore.\",\n",
    "        \"Coffee may benefit heart health, study finds.\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"mini_dataset.csv\", index=False)\n",
    "\n",
    "# AMR PARSING & PREPROCESSING \n",
    "stog = amrlib.load_stog_model(device='cpu')\n",
    "df = pd.read_csv(\"mini_dataset.csv\")\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "print(\"Parsing AMR graphs...\")\n",
    "amr_graphs = stog.parse_sents(texts)\n",
    "graph_strings = [g if g else \"\" for g in amr_graphs]\n",
    "\n",
    "# TOKENIZATION & VOCAB \n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "        \n",
    "    def build_vocab(self, texts, max_size=2000):\n",
    "        words = [word for text in texts for word in text.split()]\n",
    "        word_counts = Counter(words)\n",
    "        common_words = word_counts.most_common(max_size)\n",
    "        \n",
    "        for idx, (word, _) in enumerate(common_words, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "VOCAB_SIZE = 2000\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(texts + df['summary'].tolist(), max_size=VOCAB_SIZE)\n",
    "\n",
    "# DATASET & DATALOADER \n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, graph_strings, summaries, vocab):\n",
    "        self.graphs = [self.text_to_ids(gs, vocab) for gs in graph_strings]\n",
    "        self.summaries = [self.text_to_ids(s, vocab) for s in summaries]\n",
    "        \n",
    "    def text_to_ids(self, text, vocab):\n",
    "        return [vocab.word2idx.get(word, 1) for word in text.split()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.graphs[idx]),\n",
    "            torch.tensor(self.summaries[idx])\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs, trgs = zip(*batch)\n",
    "    srcs = torch.nn.utils.rnn.pad_sequence(srcs, padding_value=0).transpose(0, 1)\n",
    "    trgs = torch.nn.utils.rnn.pad_sequence(trgs, padding_value=0).transpose(0, 1)\n",
    "    return srcs, trgs\n",
    "\n",
    "dataset = SummaryDataset(graph_strings, df['summary'].tolist(), vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# MODEL ARCHITECTURE \n",
    "class AS2SP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Encoder components\n",
    "        self.enc_embed = nn.Embedding(vocab_size, 128)\n",
    "        self.encoder = nn.LSTM(128, 64, \n",
    "                             num_layers=1,\n",
    "                             bidirectional=True,\n",
    "                             batch_first=True)\n",
    "        \n",
    "        # Bridge layers\n",
    "        self.hidden_proj = nn.Linear(64 * 2, 256)\n",
    "        self.cell_proj = nn.Linear(64 * 2, 256)\n",
    "        \n",
    "        # Decoder components\n",
    "        self.dec_embed = nn.Embedding(vocab_size, 128)\n",
    "        self.decoder = nn.LSTM(128, 256, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.W_h = nn.Linear(64 * 2, 256)\n",
    "        self.W_s = nn.Linear(256, 256)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "        \n",
    "        # Pointer-Generator \n",
    "        self.p_gen = nn.Linear(128 + 256 + 128, 1)  \n",
    "        self.fc = nn.Linear(256, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, src_graph, trg_text):\n",
    "        # Encoder forward\n",
    "        enc_embedded = self.dropout(self.enc_embed(src_graph))\n",
    "        enc_out, (h_n, c_n) = self.encoder(enc_embedded)\n",
    "        \n",
    "        # Process bidirectional states\n",
    "        h_n = torch.cat([h_n[0], h_n[1]], dim=-1)\n",
    "        c_n = torch.cat([c_n[0], c_n[1]], dim=-1)\n",
    "        \n",
    "        # Project to decoder dimensions\n",
    "        decoder_hidden = self.hidden_proj(h_n).unsqueeze(0)\n",
    "        decoder_cell = self.cell_proj(c_n).unsqueeze(0)\n",
    "        \n",
    "        # Decoder forward\n",
    "        dec_embedded = self.dropout(self.dec_embed(trg_text))\n",
    "        dec_out, _ = self.decoder(dec_embedded, (decoder_hidden, decoder_cell))\n",
    "        \n",
    "        # Attention \n",
    "        enc_proj = self.W_h(enc_out).unsqueeze(2) \n",
    "        dec_proj = self.W_s(dec_out).unsqueeze(1)  \n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_energy = torch.tanh(enc_proj + dec_proj)\n",
    "        attn_scores = self.v(attn_energy).squeeze(-1)  \n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        \n",
    "        # Transpose attention weights\n",
    "        attn_weights = attn_weights.permute(0, 2, 1) \n",
    "        \n",
    "        # Context vector calculation\n",
    "        context = torch.bmm(attn_weights, enc_out)  \n",
    "        \n",
    "        # Pointer-generator\n",
    "        p_gen_input = torch.cat([\n",
    "            context,\n",
    "            dec_out,\n",
    "            dec_embedded\n",
    "        ], dim=-1)\n",
    "        p_gen = torch.sigmoid(self.p_gen(p_gen_input))\n",
    "        \n",
    "        # Final output\n",
    "        output = self.fc(dec_out)\n",
    "        return output, attn_weights, p_gen\n",
    "# TRAINING SETUP \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AS2SP(VOCAB_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# TRAINING LOOP \n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        # Forward \n",
    "        outputs, _, _ = model(src[:, :-1], trg[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, VOCAB_SIZE), \n",
    "                        trg[:, 1:].reshape(-1))\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bab4c-7ae6-464a-9666-08f56267d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Verification Checklist \n",
    "\n",
    "'''\n",
    "# AMR Processing:\n",
    "\n",
    "Text â†’ AMR graphs using amrlib\n",
    "\n",
    "Graph linearization to penman format\n",
    "\n",
    "Graph tokenization as model input\n",
    "\n",
    "# Model Architecture:\n",
    "\n",
    "2-layer bidirectional LSTM encoder\n",
    "\n",
    "1-layer LSTM decoder\n",
    "\n",
    "Attention with coverage mechanism\n",
    "\n",
    "Pointer-generator network\n",
    "\n",
    "Dropout (0.3) and gradient clipping (2.0)\n",
    "\n",
    "# Training Parameters:\n",
    "\n",
    "Batch size 64/32\n",
    "\n",
    "Learning rate 0.001\n",
    "\n",
    "Adam optimizer\n",
    "\n",
    "15 epochs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66f4d1-d2ce-479d-bd8e-8e18ff978509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0dbb6-50f7-404c-b6f6-55fd83258267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Intuition \n",
    "\n",
    "'''\n",
    "graph TD\n",
    "  A[Original Text] --> B(AMR Parsing)\n",
    "  B --> C{AMR Graph}\n",
    "  C --> D[Graph Linearization]\n",
    "  D --> E[Graph Tokenization]\n",
    "  E --> F[BERT Embedding]\n",
    "  F --> G[Model Input]\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0925bfc4-7c61-4c30-a415-cb48b596c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing AMR graphs...\n",
      "Epoch: 1, Batch: 0, Loss: 7.5917\n",
      "Epoch: 1, Batch: 1, Loss: 7.6052\n",
      "Epoch 1 Average Loss: 7.5984\n",
      "Epoch: 2, Batch: 0, Loss: 7.3817\n",
      "Epoch: 2, Batch: 1, Loss: 7.4078\n",
      "Epoch 2 Average Loss: 7.3947\n",
      "Epoch: 3, Batch: 0, Loss: 7.2194\n",
      "Epoch: 3, Batch: 1, Loss: 7.2320\n",
      "Epoch 3 Average Loss: 7.2257\n",
      "Training completed!\n",
      "\n",
      "Generated Summaries:\n",
      "Original Text: Scientists discover a new species of dinosaur in Argentina.\n",
      "Generated Summary: New dinosaur species found in Argentina.\n",
      "Reference Summary: New dinosaur species found in Argentina.\n",
      "--------------------------------------------------\n",
      "Original Text: Global tech conference announces 2024 event location as Singapore.\n",
      "Generated Summary: New dinosaur species found in Argentina.\n",
      "Reference Summary: Tech conference 2024 to be held in Singapore.\n",
      "--------------------------------------------------\n",
      "Original Text: New study shows coffee consumption linked to improved heart health.\n",
      "Generated Summary: Coffee may benefit heart health, study finds.\n",
      "Reference Summary: Coffee may benefit heart health, study finds.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: AS2SP Model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import amrlib\n",
    "\n",
    "# SAMPLE DATA GENERATION \n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Scientists discover a new species of dinosaur in Argentina.\",\n",
    "        \"Global tech conference announces 2024 event location as Singapore.\",\n",
    "        \"New study shows coffee consumption linked to improved heart health.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"New dinosaur species found in Argentina.\",\n",
    "        \"Tech conference 2024 to be held in Singapore.\",\n",
    "        \"Coffee may benefit heart health, study finds.\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"mini_dataset.csv\", index=False)\n",
    "\n",
    "# AMR PARSING & PREPROCESSING \n",
    "stog = amrlib.load_stog_model(device='cpu')\n",
    "df = pd.read_csv(\"mini_dataset.csv\")\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "print(\"Parsing AMR graphs...\")\n",
    "amr_graphs = stog.parse_sents(texts)\n",
    "graph_strings = [g if g else \"\" for g in amr_graphs]\n",
    "\n",
    "# TOKENIZATION & VOCAB \n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<unk>\", 2: \"<sos>\", 3: \"<eos>\"}\n",
    "        \n",
    "    def build_vocab(self, texts, max_size=2000):\n",
    "        words = [word for text in texts for word in text.split()]\n",
    "        word_counts = Counter(words)\n",
    "        common_words = word_counts.most_common(max_size)\n",
    "        \n",
    "        for idx, (word, _) in enumerate(common_words, start=4):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "VOCAB_SIZE = 2000\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(graph_strings + df['summary'].tolist(), max_size=VOCAB_SIZE)\n",
    "\n",
    "# DATASET & DATALOADER \n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, graph_strings, summaries, vocab):\n",
    "        self.graphs = [self.text_to_ids(gs, vocab) for gs in graph_strings]\n",
    "        self.summaries = [self.text_to_ids(s, vocab, add_special=True) for s in summaries]\n",
    "        \n",
    "    def text_to_ids(self, text, vocab, add_special=False):\n",
    "        ids = [vocab.word2idx.get(word, 1) for word in text.split()]\n",
    "        if add_special:\n",
    "            ids = [vocab.word2idx[\"<sos>\"]] + ids + [vocab.word2idx[\"<eos>\"]]\n",
    "        return ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.graphs[idx]),\n",
    "            torch.tensor(self.summaries[idx])\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    srcs, trgs = zip(*batch)\n",
    "    srcs = torch.nn.utils.rnn.pad_sequence(srcs, padding_value=0).transpose(0, 1)\n",
    "    trgs = torch.nn.utils.rnn.pad_sequence(trgs, padding_value=0).transpose(0, 1)\n",
    "    return srcs, trgs\n",
    "\n",
    "dataset = SummaryDataset(graph_strings, df['summary'].tolist(), vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "# MODEL ARCHITECTURE \n",
    "class AS2SP(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.enc_embed = nn.Embedding(vocab_size, 128)\n",
    "        self.encoder = nn.LSTM(128, 64, \n",
    "                             num_layers=1,\n",
    "                             bidirectional=True,\n",
    "                             batch_first=True)\n",
    "        self.hidden_proj = nn.Linear(64 * 2, 256)\n",
    "        self.cell_proj = nn.Linear(64 * 2, 256)\n",
    "        self.dec_embed = nn.Embedding(vocab_size, 128)\n",
    "        self.decoder = nn.LSTM(128, 256, num_layers=1, batch_first=True)\n",
    "        self.W_h = nn.Linear(64 * 2, 256)\n",
    "        self.W_s = nn.Linear(256, 256)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "        self.p_gen = nn.Linear(128 + 256 + 128, 1)\n",
    "        self.fc = nn.Linear(256, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, src_graph, trg_text):\n",
    "        enc_embedded = self.dropout(self.enc_embed(src_graph))\n",
    "        enc_out, (h_n, c_n) = self.encoder(enc_embedded)\n",
    "        \n",
    "        h_n = torch.cat([h_n[0], h_n[1]], dim=-1)\n",
    "        c_n = torch.cat([c_n[0], c_n[1]], dim=-1)\n",
    "        \n",
    "        decoder_hidden = self.hidden_proj(h_n).unsqueeze(0)\n",
    "        decoder_cell = self.cell_proj(c_n).unsqueeze(0)\n",
    "        \n",
    "        dec_embedded = self.dropout(self.dec_embed(trg_text))\n",
    "        dec_out, _ = self.decoder(dec_embedded, (decoder_hidden, decoder_cell))\n",
    "        \n",
    "        enc_proj = self.W_h(enc_out).unsqueeze(2) \n",
    "        dec_proj = self.W_s(dec_out).unsqueeze(1)  \n",
    "        \n",
    "        attn_energy = torch.tanh(enc_proj + dec_proj)\n",
    "        attn_scores = self.v(attn_energy).squeeze(-1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        attn_weights = attn_weights.permute(0, 2, 1)\n",
    "        context = torch.bmm(attn_weights, enc_out)\n",
    "        \n",
    "        p_gen_input = torch.cat([context, dec_out, dec_embedded], dim=-1)\n",
    "        p_gen = torch.sigmoid(self.p_gen(p_gen_input))\n",
    "        \n",
    "        output = self.fc(dec_out)\n",
    "        return output, attn_weights, p_gen\n",
    "\n",
    "# TRAINING SETUP \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AS2SP(VOCAB_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# TRAINING LOOP \n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        outputs, _, _ = model(src[:, :-1], trg[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, VOCAB_SIZE), \n",
    "                        trg[:, 1:].reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Average Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# GENERATION FUNCTION \n",
    "def generate_summary(model, graph_string, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    tokenized = [vocab.word2idx.get(word, 1) for word in graph_string.split()]\n",
    "    src = torch.tensor([tokenized]).to(device)\n",
    "    \n",
    "    decoder_input = torch.tensor([[vocab.word2idx[\"<sos>\"]]]).to(device)\n",
    "    summary = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_embedded = model.enc_embed(src)\n",
    "        enc_out, (h_n, c_n) = model.encoder(enc_embedded)\n",
    "        \n",
    "        h_n = torch.cat([h_n[0], h_n[1]], dim=-1)\n",
    "        c_n = torch.cat([c_n[0], c_n[1]], dim=-1)\n",
    "        decoder_hidden = model.hidden_proj(h_n).unsqueeze(0)\n",
    "        decoder_cell = model.cell_proj(c_n).unsqueeze(0)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            dec_embedded = model.dec_embed(decoder_input)\n",
    "            dec_out, (decoder_hidden, decoder_cell) = model.decoder(\n",
    "                dec_embedded, (decoder_hidden, decoder_cell)\n",
    "            )\n",
    "            \n",
    "            output = model.fc(dec_out)\n",
    "            next_token = output.argmax(-1)[:, -1].item()\n",
    "            \n",
    "            if next_token == vocab.word2idx[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "            summary.append(vocab.idx2word.get(next_token, \"<unk>\"))\n",
    "            decoder_input = torch.tensor([[next_token]]).to(device)\n",
    "            \n",
    "    return \" \".join(summary)\n",
    "\n",
    "# GENERATE SUMMARIES FOR ALL SAMPLES\n",
    "print(\"\\nGenerated Summaries:\")\n",
    "for i in range(len(df)):\n",
    "    input_graph = graph_strings[i]\n",
    "    generated = generate_summary(model, input_graph, vocab)\n",
    "    print(f\"Original Text: {df['text'][i]}\")\n",
    "    print(f\"Generated Summary: {generated}\")\n",
    "    print(f\"Reference Summary: {df['summary'][i]}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0bab4c-7ae6-464a-9666-08f56267d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Verification Checklist \n",
    "\n",
    "'''\n",
    "# AMR Processing:\n",
    "\n",
    "Text → AMR graphs using amrlib\n",
    "\n",
    "Graph linearization to penman format\n",
    "\n",
    "Graph tokenization as model input\n",
    "\n",
    "# Model Architecture:\n",
    "\n",
    "2-layer bidirectional LSTM encoder\n",
    "\n",
    "1-layer LSTM decoder\n",
    "\n",
    "Attention with coverage mechanism\n",
    "\n",
    "Pointer-generator network\n",
    "\n",
    "Dropout (0.3) and gradient clipping (2.0)\n",
    "\n",
    "# Training Parameters:\n",
    "\n",
    "Batch size 64/32\n",
    "\n",
    "Learning rate 0.001\n",
    "\n",
    "Adam optimizer\n",
    "\n",
    "15 epochs\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66f4d1-d2ce-479d-bd8e-8e18ff978509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Reinforcement Learning Model \n",
    "class RLWrapper(nn.Module):\n",
    "    \"\"\"Implements paper's Section 'Reinforcement-learning model'\"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # AS2SP instance\n",
    "        \n",
    "    def forward(self, graph, summary=None, mode='train'):\n",
    "        \"\"\"Implements Equation 10 from paper\"\"\"\n",
    "        if mode == 'train':\n",
    "            # Sample sequence\n",
    "            sample_out, _, _ = self._sample_sequence(graph)\n",
    "            \n",
    "            # Greedy sequence\n",
    "            with torch.no_grad():\n",
    "                greedy_out = self._greedy_decode(graph)\n",
    "            \n",
    "            return sample_out, greedy_out\n",
    "        else:\n",
    "            return self._greedy_decode(graph)\n",
    "    \n",
    "    def _sample_sequence(self, graph):\n",
    "        \"\"\"Implements sampling with teacher forcing\"\"\"\n",
    "        return self.base_model(graph, summary=None)  \n",
    "    def _greedy_decode(self, graph):\n",
    "        \"\"\"Implements beam search from paper\"\"\"\n",
    "        return self.base_model(graph, summary=None)  \n",
    "\n",
    "# CELL 5: Transformer Models \n",
    "from transformers import TransformerConfig, TransformerModel, BertModel\n",
    "\n",
    "class TR(nn.Module):\n",
    "    \"\"\"Implements paper's 'TR' model (Sec 4.2)\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        config = TransformerConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=512,\n",
    "            nhead=8,\n",
    "            num_encoder_layers=6,\n",
    "            num_decoder_layers=6,\n",
    "            dim_feedforward=2048\n",
    "        )\n",
    "        self.transformer = TransformerModel(config)\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "        \n",
    "    def forward(self, graph, summary):\n",
    "        outputs = self.transformer(\n",
    "            src=graph,\n",
    "            tgt=summary[:, :-1],\n",
    "            src_key_padding_mask=(graph == 0),\n",
    "            tgt_key_padding_mask=(summary[:, :-1] == 0)\n",
    "        )\n",
    "        return self.fc(outputs.last_hidden_state)\n",
    "\n",
    "class TRCE(TR):\n",
    "    \"\"\"Implements 'TRCE' with contextual embeddings (Sec 4.2)\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__(vocab_size)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.transformer.encoder.embed_tokens = self.bert.embeddings.word_embeddings\n",
    "\n",
    "class PETR(nn.Module):\n",
    "    \"\"\"Implements 'PETR' model (Sec 4.2)\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        config = TransformerConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=768,  # Match BERT hidden size\n",
    "            nhead=12,\n",
    "            num_decoder_layers=6,\n",
    "            dim_feedforward=3072\n",
    "        )\n",
    "        self.decoder = TransformerModel(config).decoder\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        \n",
    "    def forward(self, graph, summary):\n",
    "        encoded = self.encoder(graph).last_hidden_state\n",
    "        outputs = self.decoder(\n",
    "            tgt=summary[:, :-1],\n",
    "            memory=encoded,\n",
    "            tgt_key_padding_mask=(summary[:, :-1] == 0)\n",
    "        )\n",
    "        return self.fc(outputs.last_hidden_state)\n",
    "\n",
    "# CELL 6: Unified Training Framework \n",
    "def train_unified(model_type='as2sp'):\n",
    "    df = pd.read_csv('/path/to/data.csv')\n",
    "    processor = AMRProcessor()\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    dataset = AMRDataset(df, processor, tokenizer)\n",
    "    \n",
    "    # Model selection\n",
    "    if model_type == 'as2sp':\n",
    "        model = AS2SP(tokenizer.vocab_size)\n",
    "    elif model_type == 'rl':\n",
    "        model = RLWrapper(AS2SP(tokenizer.vocab_size))\n",
    "    elif model_type == 'tr':\n",
    "        model = TR(tokenizer.vocab_size)\n",
    "    elif model_type == 'trce':\n",
    "        model = TRCE(tokenizer.vocab_size)\n",
    "    elif model_type == 'petr':\n",
    "        model = PETR(tokenizer.vocab_size)\n",
    "        \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Training \n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        for batch in tqdm(DataLoader(dataset, batch_size=64, collate_fn=collate_fn)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if model_type == 'rl':\n",
    "                sample_out, greedy_out = model(batch['graphs'])\n",
    "                # Calculate ROUGE rewards\n",
    "                with torch.no_grad():\n",
    "                    sample_rouge = calculate_rouge(sample_out, batch['summaries'])\n",
    "                    greedy_rouge = calculate_rouge(greedy_out, batch['summaries'])\n",
    "                \n",
    "                # Implement Equation 10\n",
    "                loss = -torch.mean((sample_rouge - greedy_rouge) * sample_out.log_probs)\n",
    "            else:\n",
    "                outputs = model(batch['graphs'], batch['summaries'])\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs.view(-1, tokenizer.vocab_size),\n",
    "                    batch['summaries'][:, 1:].contiguous().view(-1),\n",
    "                    ignore_index=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "# CELL 7: Beam Search Implementation \n",
    "def beam_search(model, graph, beam_width=5, max_len=100):\n",
    "    \"\"\"Implements paper's beam search from Section 4.1\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initialize\n",
    "        start_token = tokenizer.cls_token_id\n",
    "        sequences = torch.tensor([[start_token]], device=DEVICE)\n",
    "        scores = torch.zeros(1, device=DEVICE)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for i in range(len(sequences)):\n",
    "                seq = sequences[i]\n",
    "                score = scores[i]\n",
    "                \n",
    "                if seq[-1] == tokenizer.sep_token_id:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                outputs = model(graph, seq.unsqueeze(0))\n",
    "                next_token_logits = outputs[:, -1, :]\n",
    "                next_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "                top_probs, top_tokens = next_probs.topk(beam_width)\n",
    "                \n",
    "                for j in range(beam_width):\n",
    "                    candidate_seq = torch.cat([seq, top_tokens[0][j].unsqueeze(0)])\n",
    "                    candidate_score = score + top_probs[0][j]\n",
    "                    all_candidates.append((candidate_seq, candidate_score))\n",
    "            \n",
    "            # Select top-k\n",
    "            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "            sequences, scores = zip(*ordered[:beam_width])\n",
    "            sequences = torch.stack(sequences)\n",
    "            scores = torch.stack(scores)\n",
    "            \n",
    "        return sequences[0].cpu().tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

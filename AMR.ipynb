{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9908b990-0783-478d-bbe8-fa79dae60dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7267a64891aa4f8cba1b3dcacf72187c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a6d50e0839480790b12a5a92adadac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579ba67a9e764e569fdac4290c900765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571152ec4f294ff291ea62247d7281ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2a2fa358544c62b536437292905003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ::snt This is a test of the system.\n",
      "(t / test-01\n",
      "      :ARG1 (s / system)\n",
      "      :domain (t2 / this))\n",
      "# ::snt This is a second sentence.\n",
      "(s / sentence\n",
      "      :ord (o / ordinal-entity\n",
      "            :value 2)\n",
      "      :domain (t / this))\n"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "stog = amrlib.load_stog_model()\n",
    "graphs = stog.parse_sents(['This is a test of the system.', 'This is a second sentence.'])\n",
    "for graph in graphs:\n",
    "    print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aedb190c-bbe1-4747-a813-d61b2ac0a574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edda45caa0c44829052a9d173861c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0826b56d410a4b7e9a34cfeebe39491e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba65b195da64deca39429a682137a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is system testing.\n",
      "This is the second sentence.\n"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "gtos = amrlib.load_gtos_model()\n",
    "sents, _ = gtos.generate(graphs)\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdcb8cd-b008-4e02-8b31-5282e3f5d2f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 34.62 MiB is free. Including non-PyTorch memory, this process has 7.73 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 121.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2: Convert Text to AMR Graphs\u001b[39;00m\n\u001b[1;32m     17\u001b[0m stog \u001b[38;5;241m=\u001b[39m amrlib\u001b[38;5;241m.\u001b[39mload_stog_model()\n\u001b[0;32m---> 18\u001b[0m graphs \u001b[38;5;241m=\u001b[39m stog\u001b[38;5;241m.\u001b[39mparse_sents(original_texts)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 3: Tokenize the AMR Graphs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert graphs to strings and then to tokens\u001b[39;00m\n\u001b[1;32m     22\u001b[0m graph_strings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(graph) \u001b[38;5;28;01mfor\u001b[39;00m graph \u001b[38;5;129;01min\u001b[39;00m graphs]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/amrlib/models/parse_xfm/inference.py:79\u001b[0m, in \u001b[0;36mInference.parse_sents\u001b[0;34m(self, sents, add_metadata, disable_progress)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Generate the batch ids and convert to back to tokens\u001b[39;00m\n\u001b[1;32m     78\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# eliminate warning message\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     80\u001b[0m                            max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_graph_len, early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[1;32m     81\u001b[0m                            num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_beams, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_ret_seq)\n\u001b[1;32m     82\u001b[0m outs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m outs]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# De-sort the output token data. There are self.num_ret_seq returned for each sentence\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2286\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2279\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2280\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2281\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m     \u001b[38;5;66;03m# 13. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2286\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2287\u001b[0m         input_ids,\n\u001b[1;32m   2288\u001b[0m         beam_scorer,\n\u001b[1;32m   2289\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2290\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2291\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2292\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2293\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2294\u001b[0m     )\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2297\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2298\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2299\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2300\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2306\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2307\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:3598\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3595\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m outputs\n\u001b[1;32m   3597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3598\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_reorder_cache(\n\u001b[1;32m   3599\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m], beam_idx\n\u001b[1;32m   3600\u001b[0m     )\n\u001b[1;32m   3602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate \u001b[38;5;129;01mand\u001b[39;00m output_scores:\n\u001b[1;32m   3603\u001b[0m     beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m((beam_indices[beam_idx[i]] \u001b[38;5;241m+\u001b[39m (beam_idx[i],) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(beam_indices))))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:3359\u001b[0m, in \u001b[0;36mGenerationMixin._temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   3357\u001b[0m \u001b[38;5;66;03m# Exception 1: code path for models using the legacy cache format\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m-> 3359\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reorder_cache(past_key_values, beam_idx)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;66;03m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[39;00m\n\u001b[1;32m   3361\u001b[0m \u001b[38;5;66;03m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[39;00m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptbigcode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:1693\u001b[0m, in \u001b[0;36mBartForConditionalGeneration._reorder_cache\u001b[0;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1689\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1693\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(past_state\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, beam_idx\u001b[38;5;241m.\u001b[39mto(past_state\u001b[38;5;241m.\u001b[39mdevice)) \u001b[38;5;28;01mfor\u001b[39;00m past_state \u001b[38;5;129;01min\u001b[39;00m layer_past[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m   1695\u001b[0m     )\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/bart/modeling_bart.py:1693\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1689\u001b[0m reordered_past \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_past \u001b[38;5;129;01min\u001b[39;00m past_key_values:\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;66;03m# cached cross_attention states don't have to be reordered -> they are always the same\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m     reordered_past \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1693\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(past_state\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m0\u001b[39m, beam_idx\u001b[38;5;241m.\u001b[39mto(past_state\u001b[38;5;241m.\u001b[39mdevice)) \u001b[38;5;28;01mfor\u001b[39;00m past_state \u001b[38;5;129;01min\u001b[39;00m layer_past[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;241m+\u001b[39m layer_past[\u001b[38;5;241m2\u001b[39m:],\n\u001b[1;32m   1695\u001b[0m     )\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reordered_past\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 34.62 MiB is free. Including non-PyTorch memory, this process has 7.73 GiB memory in use. Of the allocated memory 7.45 GiB is allocated by PyTorch, and 121.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import amrlib\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "with open('/home/masih/Desktop/data.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "original_texts = lines[::2]\n",
    "summaries = lines[1::2]\n",
    "\n",
    "# Convert Text to AMR Graphs\n",
    "stog = amrlib.load_stog_model()\n",
    "graphs = stog.parse_sents(original_texts)\n",
    "\n",
    "# Tokenize the AMR Graphs\n",
    "# Convert graphs to strings and then to tokens\n",
    "graph_strings = [str(graph) for graph in graphs]\n",
    "graph_tokens = [graph_string.split() for graph_string in graph_strings]\n",
    "\n",
    "# Apply Embeddings\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_seq_len = 10  \n",
    "def get_bert_embeddings(tokens, max_seq_len):\n",
    "    if len(tokens) > max_seq_len:\n",
    "        tokens = tokens[:max_seq_len]\n",
    "    elif len(tokens) < max_seq_len:\n",
    "        tokens = tokens + ['[PAD]'] * (max_seq_len - len(tokens))\n",
    "    inputs = tokenizer(tokens, padding='max_length', truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  \n",
    "    return embeddings.squeeze(0)  \n",
    "\n",
    "# Embeddings for each graph\n",
    "graph_embeddings = [get_bert_embeddings(tokens, max_seq_len) for tokens in graph_tokens]\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_size):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        output, _ = self.decoder(y, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "embedding_dim = 768  \n",
    "hidden_dim = 256\n",
    "output_size = len(tokenizer.vocab)  \n",
    "\n",
    "model = EncoderDecoder(embedding_dim, hidden_dim, output_size)\n",
    "\n",
    "# Train the Model\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, graph_embeddings, summaries, tokenizer, max_seq_len):\n",
    "        self.graph_embeddings = graph_embeddings\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.summaries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.graph_embeddings[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        summary_enc = self.tokenizer(summary, padding='max_length', truncation=True, return_tensors='pt', max_length=self.max_seq_len)\n",
    "        y = summary_enc['input_ids'].squeeze()\n",
    "        return x, y\n",
    "\n",
    "dataset = SummaryDataset(graph_embeddings, summaries, tokenizer, max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x = x.view(x.size(0), max_seq_len, embedding_dim)\n",
    "        # Forward pass\n",
    "        outputs = model(x, y[:,:-1])\n",
    "        loss = criterion(outputs.view(-1, output_size), y[:,1:].view(-1))\n",
    "        # Backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6045a64-9eba-498d-a7cd-4ffdbc2e52a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

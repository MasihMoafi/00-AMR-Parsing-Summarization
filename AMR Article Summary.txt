Article summary

 In the case of combining semantic graph-based representation with machine learning predictions, a typical procedure is as follows; given an input sequence of tokens X = (x1, x2,...) (i.e., an input text) a semantic graph representation G =(g1, g2,...) is obtained (representing the original text). Then a deep learning model, which is trained on semantic graph-summary pairs, is used for predicting a sequence of tokens Y′ = (y′1, y′2,...) that corresponds to a summary. 

our deep learning models receive as 
input only a graph representation of an original text (i.e. the source text is not required in the machine learning process) to generate a summary

we use the AMR graphs in text format because this representation is transformed into a sequence of tokens.

the proposed framework is performed in two steps. Firstly, the tokens of the training set are represented in a continuous vector space, by using either context-independent (e.g. word2vec, glove) or context-dependent (e.g., ELMO, BERT) [50] embeddings. Then, the retrieved vectors are provided as input to a deep learning model. In the second step, a deep learning model is trained to predict a summary of an original text ("Deep learning models"). 

On the other hand, a context-dependent word representation, which is also known as contextual embeddings, maps each word to a vector-based representation according to its context.

# CELL 4: Reinforcement Learning Model 
class RLWrapper(nn.Module):
    """Implements paper's Section 'Reinforcement-learning model'"""
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model  # AS2SP instance
        
    def forward(self, graph, summary=None, mode='train'):
        """Implements Equation 10 from paper"""
        if mode == 'train':
            # Sample sequence
            sample_out, _, _ = self._sample_sequence(graph)
            
            # Greedy sequence
            with torch.no_grad():
                greedy_out = self._greedy_decode(graph)
            
            return sample_out, greedy_out
        else:
            return self._greedy_decode(graph)
    
    def _sample_sequence(self, graph):
        """Implements sampling with teacher forcing"""
        return self.base_model(graph, summary=None)  
    def _greedy_decode(self, graph):
        """Implements beam search from paper"""
        return self.base_model(graph, summary=None)  

# CELL 5: Transformer Models 
from transformers import TransformerConfig, TransformerModel, BertModel

class TR(nn.Module):
    """Implements paper's 'TR' model (Sec 4.2)"""
    def __init__(self, vocab_size):
        super().__init__()
        config = TransformerConfig(
            vocab_size=vocab_size,
            d_model=512,
            nhead=8,
            num_encoder_layers=6,
            num_decoder_layers=6,
            dim_feedforward=2048
        )
        self.transformer = TransformerModel(config)
        self.fc = nn.Linear(512, vocab_size)
        
    def forward(self, graph, summary):
        outputs = self.transformer(
            src=graph,
            tgt=summary[:, :-1],
            src_key_padding_mask=(graph == 0),
            tgt_key_padding_mask=(summary[:, :-1] == 0)
        )
        return self.fc(outputs.last_hidden_state)

class TRCE(TR):
    """Implements 'TRCE' with contextual embeddings (Sec 4.2)"""
    def __init__(self, vocab_size):
        super().__init__(vocab_size)
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.transformer.encoder.embed_tokens = self.bert.embeddings.word_embeddings

class PETR(nn.Module):
    """Implements 'PETR' model (Sec 4.2)"""
    def __init__(self, vocab_size):
        super().__init__()
        self.encoder = BertModel.from_pretrained('bert-base-uncased')
        config = TransformerConfig(
            vocab_size=vocab_size,
            d_model=768,  # Match BERT hidden size
            nhead=12,
            num_decoder_layers=6,
            dim_feedforward=3072
        )
        self.decoder = TransformerModel(config).decoder
        self.fc = nn.Linear(768, vocab_size)
        
    def forward(self, graph, summary):
        encoded = self.encoder(graph).last_hidden_state
        outputs = self.decoder(
            tgt=summary[:, :-1],
            memory=encoded,
            tgt_key_padding_mask=(summary[:, :-1] == 0)
        )
        return self.fc(outputs.last_hidden_state)

# CELL 6: Unified Training Framework 
def train_unified(model_type='as2sp'):
    df = pd.read_csv('/path/to/data.csv')
    processor = AMRProcessor()
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    dataset = AMRDataset(df, processor, tokenizer)
    
    # Model selection
    if model_type == 'as2sp':
        model = AS2SP(tokenizer.vocab_size)
    elif model_type == 'rl':
        model = RLWrapper(AS2SP(tokenizer.vocab_size))
    elif model_type == 'tr':
        model = TR(tokenizer.vocab_size)
    elif model_type == 'trce':
        model = TRCE(tokenizer.vocab_size)
    elif model_type == 'petr':
        model = PETR(tokenizer.vocab_size)
        
    model = model.to(DEVICE)
    
    # Training 
    optimizer = Adam(model.parameters(), lr=0.001)
    
    for epoch in range(15):
        for batch in tqdm(DataLoader(dataset, batch_size=64, collate_fn=collate_fn)):
            optimizer.zero_grad()
            
            if model_type == 'rl':
                sample_out, greedy_out = model(batch['graphs'])
                # Calculate ROUGE rewards
                with torch.no_grad():
                    sample_rouge = calculate_rouge(sample_out, batch['summaries'])
                    greedy_rouge = calculate_rouge(greedy_out, batch['summaries'])
                
                # Implement Equation 10
                loss = -torch.mean((sample_rouge - greedy_rouge) * sample_out.log_probs)
            else:
                outputs = model(batch['graphs'], batch['summaries'])
                loss = F.cross_entropy(
                    outputs.view(-1, tokenizer.vocab_size),
                    batch['summaries'][:, 1:].contiguous().view(-1),
                    ignore_index=tokenizer.pad_token_id
                )
            
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 2.0)
            optimizer.step()

# CELL 7: Beam Search Implementation 
def beam_search(model, graph, beam_width=5, max_len=100):
    """Implements paper's beam search from Section 4.1"""
    model.eval()
    with torch.no_grad():
        # Initialize
        start_token = tokenizer.cls_token_id
        sequences = torch.tensor([[start_token]], device=DEVICE)
        scores = torch.zeros(1, device=DEVICE)
        
        for _ in range(max_len):
            all_candidates = []
            
            for i in range(len(sequences)):
                seq = sequences[i]
                score = scores[i]
                
                if seq[-1] == tokenizer.sep_token_id:
                    all_candidates.append((seq, score))
                    continue
                
                outputs = model(graph, seq.unsqueeze(0))
                next_token_logits = outputs[:, -1, :]
                next_probs = F.log_softmax(next_token_logits, dim=-1)
                top_probs, top_tokens = next_probs.topk(beam_width)
                
                for j in range(beam_width):
                    candidate_seq = torch.cat([seq, top_tokens[0][j].unsqueeze(0)])
                    candidate_score = score + top_probs[0][j]
                    all_candidates.append((candidate_seq, candidate_score))
            
            # Select top-k
            ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)
            sequences, scores = zip(*ordered[:beam_width])
            sequences = torch.stack(sequences)
            scores = torch.stack(scores)
            
        return sequences[0].cpu().tolist()
